{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcE39zEwyuJ7"
   },
   "source": [
    "# CSCE 421 :: Machine Learning :: Texas A&M University :: Spring 2023\n",
    "\n",
    "# Homework 6 (HW-6)\n",
    "**Name:**  \n",
    "**UIN:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifUFY68YyuJ_"
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "In this assignment, you'll be coding up a convolutional neural network from scratch to classify images using PyTorch.  \n",
    "\n",
    "### Instructions\n",
    "- Install PyTorch following the instructions [here](https://pytorch.org/).\n",
    "- Install the [`torchinfo` package](https://github.com/TylerYep/torchinfo) to visualize the network architecture and the number of parameters. The maximum number of parameters you are allowed to use for your network is **100,000**. \n",
    "- You are required to complete the functions defined in the code blocks following each question. Fill out sections of the code marked `\"YOUR CODE HERE\"`.\n",
    "- You're free to add any number of methods within each class.\n",
    "- You may also add any number of additional code blocks that you deem necessary. \n",
    "- Once you've filled out your solutions, submit the notebook on Canvas.\n",
    "- Do **NOT** forget to type in your name and UIN at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43mQs43ryuKA"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the libraries you need based on your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_0NNNwDRyuKA"
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9g-7MOVTyuKB"
   },
   "source": [
    "In this assignment, we will use the Fashion-MNIST dataset. Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.  \n",
    "\n",
    "### Data\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.  \n",
    "\n",
    "### Labels\n",
    "\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "| Label | Description |\n",
    "|-------|-------------|\n",
    "| 0     | T-shirt/top |\n",
    "| 1     | Trouser     |\n",
    "| 2     | Pullover    |\n",
    "| 3     | Dress       |\n",
    "| 4     | Coat        |\n",
    "| 5     | Sandal      |\n",
    "| 6     | Shirt       |\n",
    "| 7     | Sneaker     |\n",
    "| 8     | Bag         |\n",
    "| 9     | Ankle boot  |\n",
    "\n",
    "Fashion-MNIST is included in the `torchvision` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q6nztLS7yuKC"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z_uzCztNyuKD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to MNIST_data/FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:28<00:00, 933897.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/FashionMNIST\\raw\\train-images-idx3-ubyte.gz to MNIST_data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to MNIST_data/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 206465.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to MNIST_data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to MNIST_data/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:05<00:00, 810297.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to MNIST_data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to MNIST_data/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 1300739.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to MNIST_data/FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform to normalize the data and convert to a tensor\n",
    "transform = Compose([ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "# Download the data\n",
    "dataset = FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S1W1ktCyuKD"
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5JfHJvzyuKE"
   },
   "source": [
    "Let's take a look at the classes in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yZ2JquNdyuKE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QU9EQWFoyuKE"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_example(img, label):\n",
    "    print('Label: {} ({})'.format(dataset.classes[label], label))\n",
    "    plt.imshow(img.squeeze(), cmap='Greys_r')\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TovSQLF6yuKF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Dress (3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(*dataset[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lDiRVqLsyuKF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Sneaker (7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(*dataset[20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GMSy_GHyuKF"
   },
   "source": [
    "## Creating Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_MEJjfYyuKF"
   },
   "source": [
    "The `split_indices` function takes in the size of the entire dataset, `n`, the fraction of data to be used as validation set, `val_frac`, and the random seed and returns the indices of the data points to be added to the validation dataset.  \n",
    "\n",
    "**Choose a suitable fraction for your validation set and experiment with the seed. Remember that the better your validation set, the higher the chances that your model would do well on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kho3emIRyuKF"
   },
   "outputs": [],
   "source": [
    "def split_indices(n, val_frac, seed):\n",
    "    # Determine the size of the validation set\n",
    "    n_val = int(val_frac * n)\n",
    "    np.random.seed(seed)\n",
    "    # Create random permutation between 0 to n-1\n",
    "    idxs = np.random.permutation(n)\n",
    "    # Pick first n_val indices for validation set\n",
    "    return idxs[n_val:], idxs[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NC7a43-ZyuKG"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#   YOUR CODE BELOW  #\n",
    "######################\n",
    "val_frac =  .4## Set the fraction for the validation set\n",
    "rand_seed =  ## Set the random seed\n",
    "\n",
    "train_indices, val_indices = split_indices(len(dataset), val_frac, rand_seed)\n",
    "print(\"#samples in training set: {}\".format(len(train_indices)))\n",
    "print(\"#samples in validation set: {}\".format(len(val_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JbJS5GByuKG"
   },
   "source": [
    "Next, we make use of the built-in dataloaders in PyTorch to create iterables of our our training and validation sets. This helps in avoiding fitting the whole dataset into memory and only loads a batch of the data that we can decide. \n",
    "\n",
    "**Set the `batch_size` depending on the hardware resource (GPU/CPU RAM) you are using for the assignment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xn2qrDnkyuKG"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQwCiHOhyuKG"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#   YOUR CODE BELOW  #\n",
    "######################\n",
    "batch_size =  ## Set the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gR9bGnYyuKG"
   },
   "outputs": [],
   "source": [
    "# Training sampler and data loader\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_dl = DataLoader(dataset,\n",
    "                     batch_size,\n",
    "                     sampler=train_sampler)\n",
    "\n",
    "# Validation sampler and data loader\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "val_dl = DataLoader(dataset,\n",
    "                   batch_size,\n",
    "                   sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZ49pxx9yuKG"
   },
   "source": [
    "Plot images in a sample batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRUpyKEryuKG"
   },
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(10,10))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images, 8).permute(1, 2, 0), cmap='Greys_r')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fg0muQ6jyuKH"
   },
   "outputs": [],
   "source": [
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PA4syzZ0yuKH"
   },
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6tp2iP8yuKH"
   },
   "source": [
    "Create your model by defining the network architecture in the `ImageClassifierNet` class.  \n",
    "**NOTE: The number of parameters in your network must be $\\leq$ 100,000.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neHUVjlyyuKH"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vy-ENg1LyuKH"
   },
   "outputs": [],
   "source": [
    "class ImageClassifierNet(nn.Module):\n",
    "    def __init__(self, n_channels=3):\n",
    "        super(ImageClassifierNet, self).__init__()\n",
    "        ######################\n",
    "        #   YOUR CODE HERE   #\n",
    "        ######################\n",
    "        \n",
    "    def forward(self, X):\n",
    "        ######################\n",
    "        #   YOUR CODE HERE   #\n",
    "        ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfWFN9TFyuKH"
   },
   "outputs": [],
   "source": [
    "model = ImageClassifierNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tU88zUSyuKH"
   },
   "source": [
    "The following code block prints your network architecture. It also shows the total number of parameters in your network (see `Total params`).  \n",
    "\n",
    "**NOTE: The total number of parameters in your model should be <= 100,000.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeCRnkERyuKH"
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=(batch_size, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GPSVM59uEea"
   },
   "source": [
    "## Check Model Requirements status\n",
    "\n",
    "The code block below is to check if your model meets the requirements specefied \n",
    "\n",
    "Run the cell to check the status of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdRVnd7BuEDl"
   },
   "outputs": [],
   "source": [
    "def check_model(model):\n",
    "  first_layer = list(model.children())[0]\n",
    "  last_layer  = list(model.children())[-1]\n",
    "  num_params = sum(p.numel() for p in model.parameters())\n",
    "  has_conv1d = False\n",
    "  has_maxpool2d = False\n",
    "  first_layer_status= False\n",
    "  last_layer_status = False\n",
    "\n",
    "  if isinstance(first_layer, nn.Sequential):\n",
    "        if isinstance(first_layer[0], nn.Conv2d) and first_layer[0].in_channels == 1:\n",
    "          first_layer_status= True\n",
    "  else: \n",
    "    if isinstance(first_layer, nn.Conv2d) and first_layer.in_channels == 1:\n",
    "      first_layer_status= True\n",
    "\n",
    "  if isinstance(last_layer, nn.Sequential):\n",
    "    if isinstance(last_layer[-1], nn.Linear) and last_layer[-1].out_features == 10:\n",
    "      last_layer_status = True\n",
    "  else:\n",
    "    print(type(last_layer))\n",
    "    if isinstance(last_layer, nn.Linear) and last_layer.out_features == 10:\n",
    "      last_layer_status = True\n",
    "\n",
    "  for layer in model.modules():\n",
    "    if isinstance(layer, nn.Conv1d):\n",
    "        has_conv1d = True\n",
    "    if isinstance(layer, nn.MaxPool2d):\n",
    "        has_maxpool2d = True\n",
    "  flag = False\n",
    "  if first_layer_status == False:\n",
    "    print(\"The very first parameter of the first layer is not 1 nn.Conv2d(1, ....)\")\n",
    "    flag = True\n",
    "  if last_layer_status == False:\n",
    "    print(\"The very last parameter of the last layer is not 10 nn.Linear(..., 10)\")\n",
    "    flag = True\n",
    "  if has_conv1d == True:\n",
    "    print(\"Using nn.Conv1d, which should not happen\")\n",
    "    flag = True\n",
    "  if has_maxpool2d == False:\n",
    "    print(\"No nn.MaxPool2d\")\n",
    "    flag = True\n",
    "  if num_params > 100000:\n",
    "    print(\"Parameters > 100000, please make the network less complex to recieve full grade\")\n",
    "  \n",
    "  if flag == False:\n",
    "    print(\"The Network looks good\") \n",
    "\n",
    "check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyrnbD-JyuKH"
   },
   "source": [
    "## Enable training on a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hijq-mSyuKH"
   },
   "source": [
    "**NOTE:** This section is necessary if you're training your model on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ag3cLTDyuKI"
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Use GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8SDHmBkyuKI"
   },
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)\n",
    "\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUpkpFcoyuKI"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_wGb9mcyuKI"
   },
   "source": [
    "Complete the `train_model` function to train your model on a dataset. Tune your network architecture and hyperparameters on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smWOXIi6yuKI"
   },
   "outputs": [],
   "source": [
    "def train_model(n_epochs, model, train_dl, val_dl, loss_fn, opt_fn, lr):\n",
    "    \"\"\"\n",
    "    Trains the model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        n_epochs: number of epochs\n",
    "        model: ImageClassifierNet object\n",
    "        train_dl: training dataloader\n",
    "        val_dl: validation dataloader\n",
    "        loss_fn: the loss function\n",
    "        opt_fn: the optimizer\n",
    "        lr: learning rate\n",
    "    \n",
    "    Returns:\n",
    "        The trained model. \n",
    "        A tuple of (model, train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "    \"\"\"\n",
    "    # Record these values the end of each epoch\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "    \n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    \n",
    "    # Validation process\n",
    "    val_loss = None\n",
    "    val_accuracy = None\n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    \n",
    "    # Record the loss and metric\n",
    "    ######################\n",
    "    #   YOUR CODE HERE   #\n",
    "    ######################\n",
    "    \n",
    "    # Print progress\n",
    "        if val_accuracy is not None:\n",
    "            print(\"Epoch {}/{}, train_loss: {:.4f}, val_loss: {:.4f}, train_accuracy: {:.4f}, val_accuracy: {:.4f}\"\n",
    "                  .format(epoch+1, n_epochs, train_loss, val_loss, train_accuracy, val_accuracy))\n",
    "        else:\n",
    "            print(\"Epoch {}/{}, train_loss: {:.4f}, train_accuracy: {:.4f}\"\n",
    "                  .format(epoch+1, n_epochs, train_loss, train_accuracy))\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLl883iGyuKI"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#   YOUR CODE BELOW  #\n",
    "######################\n",
    "num_epochs = # Max number of training epochs\n",
    "loss_fn = # Define the loss function\n",
    "opt_fn = # Select an optimizer\n",
    "lr = # Set the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtSwd0s4yuKI"
   },
   "outputs": [],
   "source": [
    "history = train_model(num_epochs, model, train_dl, val_dl, loss_fn, opt_fn, lr)\n",
    "model, train_losses, val_losses, train_accuracies, val_accuracies = history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83RGjj7MyuKJ"
   },
   "source": [
    "## Plot loss and accuracy\n",
    "\n",
    "Use the code blocks below to check if the model's accuracy increases and the model's loss values decrease throughout the training epochs. If you do not see the expected pattern, you need to go back and make sure the model's training stage is done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRMb23OVyuKJ"
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(train_accuracies, val_accuracies):\n",
    "    \"\"\"Plot accuracies\"\"\"\n",
    "    plt.plot(train_accuracies, \"-x\")\n",
    "    plt.plot(val_accuracies, \"-o\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend([\"Training\", \"Validation\"])\n",
    "    plt.title(\"Accuracy vs. No. of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrSNfm-6yuKJ"
   },
   "outputs": [],
   "source": [
    "plot_accuracy(train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SO5QQZuKyuKJ"
   },
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    \"\"\"Plot losses\"\"\"\n",
    "    plt.plot(train_losses, \"-x\")\n",
    "    plt.plot(val_losses, \"-o\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Training\", \"Validation\"])\n",
    "    plt.title(\"Loss vs. No. of Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BW28SSYyuKJ"
   },
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mcr9pxd8yuKJ"
   },
   "source": [
    "## Train a model on the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_hq90Slx_BJ"
   },
   "source": [
    "We create a new model with the architecture based on hyperparameter tuning step and train it on the whole training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeytabxJ0ygs"
   },
   "outputs": [],
   "source": [
    "new_model = ImageClassifierNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXDRhr2L_wUq"
   },
   "source": [
    "**NOTE:** The next cell is necessary if you're training your new_model on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kr4-9BUD_wUq"
   },
   "outputs": [],
   "source": [
    "to_device(new_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vWfme9Exsib"
   },
   "outputs": [],
   "source": [
    "indices, _ = split_indices(len(dataset), 0, rand_seed)\n",
    "\n",
    "sampler = SubsetRandomSampler(indices)\n",
    "dl = DataLoader(dataset, batch_size, sampler=sampler)\n",
    "dl = DeviceDataLoader(dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZxLGw9myuKJ"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#   YOUR CODE BELOW  #\n",
    "######################\n",
    "num_epochs = # Max number of training epochs\n",
    "lr = # Set the learning rate\n",
    "\n",
    "history = train_model(num_epochs, new_model, dl, [], loss_fn, opt_fn, lr)\n",
    "new_model = history[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhWY1imsyuKK"
   },
   "source": [
    "## Check Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nT4lsa5qyuKK"
   },
   "outputs": [],
   "source": [
    "def view_prediction(img, label, probs, classes):\n",
    "    \"\"\"\n",
    "    Visualize predictions.\n",
    "    \"\"\"\n",
    "    probs = probs.cpu().numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(8,15), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).cpu().numpy().squeeze(), cmap='Greys_r')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Actual: {}'.format(classes[label]))\n",
    "    ax2.barh(np.arange(10), probs)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(classes, size='small');\n",
    "    ax2.set_title('Predicted: probabilities')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWQYxhFTyuKK"
   },
   "outputs": [],
   "source": [
    "# Calculate the class probabilites (log softmax) for img\n",
    "images = iter(dl)\n",
    "for imgs, labels in images:\n",
    "    with torch.no_grad():\n",
    "        new_model.eval()\n",
    "        # Calculate the class probabilites (log softmax) for img\n",
    "        probs = torch.nn.functional.softmax(new_model(imgs[0].unsqueeze(0)), dim=1)\n",
    "        # Plot the image and probabilites\n",
    "        view_prediction(imgs[0], labels[0], probs, dataset.classes)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzNYS3LRyuKK"
   },
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxiqJXORyuKK"
   },
   "outputs": [],
   "source": [
    "torch.save(new_model, 'new_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gA5cuahMyuKK"
   },
   "source": [
    "## Compute accuracy on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EluGZMKE2i0U"
   },
   "source": [
    "Here we load the test data pickle file provided.\n",
    "\n",
    "You need to provide the path for the test_data pickle file and run the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9L6E5oAigIln"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nR1CIHF4gPZB"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#  YOUR Answer BELOW #\n",
    "######################\n",
    "test_dataset_file_path = \"test_data.pickle\"\n",
    "\n",
    "with open(test_dataset_file_path, 'rb') as f:\n",
    "    test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQxtt0J-yuKK"
   },
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_dataset, batch_size)\n",
    "test_dl = DeviceDataLoader(test_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZquvkV-yuKK"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_dl):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_test_dl = 0\n",
    "        preds, labels = [], []\n",
    "        for xb, yb in test_dl:\n",
    "            # Model output\n",
    "            y_pred = model(xb)\n",
    "            _, y_pred = torch.max(y_pred, dim=1)\n",
    "            preds.extend(y_pred)\n",
    "            labels.extend(yb)               \n",
    "            total_test_dl += len(yb)\n",
    "\n",
    "    preds, labels = torch.tensor(preds), torch.tensor(labels)\n",
    "    test_accuracy = torch.sum(preds == labels).item() / len(preds)\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XP-C6cHyuKK"
   },
   "outputs": [],
   "source": [
    "print(\"Test Accuracy = {:.4f}\".format(evaluate(new_model, test_dl)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
